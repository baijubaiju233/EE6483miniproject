{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da5e2cda",
   "metadata": {},
   "source": [
    "# Dogs vs. Cats\n",
    "\n",
    "\n",
    "This Notebook include:\n",
    "1.  Data preprocessing\n",
    "2.  Load train, validation and test datasets\n",
    "3.  Model define and training\n",
    "4.  Whole model Fine-Tuning\n",
    "5.  Predict test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40381f6d",
   "metadata": {},
   "source": [
    "### 1. Data preprocessing\n",
    "\n",
    "Define training transforms(with augmentaion) and validation transforms. This is because during testing, we want to evaluate the model's true performance on images that are \"original\" and consistent, rather than on randomly varied images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbbf32f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\83696\\.conda\\envs\\chatglm3\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data transforms defined successfully (with RandomErasing).\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset, Subset # <--- 导入 Subset\n",
    "from torch.utils.data import random_split \n",
    "from torchvision.datasets import CIFAR10\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models\n",
    "import time\n",
    "import copy\n",
    "# 1. DATA PREPROCESSING (保持不变)\n",
    "IMG_SIZE = 224\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.RandomErasing(p=0.5, scale=(0.02, 0.2), ratio=(0.3, 3.3), value=0),\n",
    "    transforms.Normalize(mean=mean, std=std)\n",
    "])\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean, std=std)\n",
    "])\n",
    "print(\"Data transforms defined successfully (with RandomErasing).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612b3c22-585b-49a7-b1b1-04698996f767",
   "metadata": {},
   "source": [
    "### 2. Load train，validation and test datasets\n",
    "\n",
    "Creat two dataLoaders: one for training and one for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "683c91f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Original split: 45000 train, 5000 val\n",
      "\n",
      "--- Simulating Data Imbalance for Task (h) ---\n",
      "Created imbalanced training set with 24788 images.\n",
      "Imbalanced class counts: [4512 4488 4468 4529 4529  458  488  439  434  443]\n",
      "\n",
      "--- Applying Solution 1: WeightedRandomSampler ---\n",
      "train_loader created using WeightedRandomSampler.\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 64 \n",
    "DATA_DIR = './dataset' \n",
    "\n",
    "full_train_dataset = torchvision.datasets.CIFAR10(\n",
    "    root=DATA_DIR, train=True, download=True, transform=train_transform)\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(\n",
    "    root=DATA_DIR, train=False, download=True, transform=test_transform)\n",
    "\n",
    "VAL_SPLIT_SIZE = 5000\n",
    "TRAIN_SPLIT_SIZE = len(full_train_dataset) - VAL_SPLIT_SIZE\n",
    "train_subset, val_subset = random_split(\n",
    "    full_train_dataset, \n",
    "    [TRAIN_SPLIT_SIZE, VAL_SPLIT_SIZE],\n",
    "    generator=torch.Generator().manual_seed(42)\n",
    ")\n",
    "\n",
    "print(f\"Original split: {len(train_subset)} train, {len(val_subset)} val\")\n",
    "\n",
    "# Simulate data imbalance\n",
    "# Keep 5 categories (0-4) unchanged, but reduce the amount of data in the other 5 categories (5-9) by 90%\n",
    "\n",
    "print(\"\\n--- Simulating Data Imbalance for Task (h) ---\")\n",
    "# {0: 1.0, 1: 1.0, 2: 1.0, 3: 1.0, 4: 1.0,  \n",
    "#  5: 0.1, 6: 0.1, 7: 0.1, 8: 0.1, 9: 0.1} \n",
    "imbalance_ratios = {i: 1.0 if i < 5 else 0.1 for i in range(10)}\n",
    "\n",
    "original_train_indices = np.array(train_subset.indices)\n",
    "original_train_labels = np.array(full_train_dataset.targets)[original_train_indices]\n",
    "\n",
    "imbalanced_indices = []\n",
    "for i, (idx, label) in enumerate(zip(original_train_indices, original_train_labels)):\n",
    "    if np.random.rand() < imbalance_ratios[label]:\n",
    "        imbalanced_indices.append(idx)\n",
    "\n",
    "imbalanced_train_dataset = Subset(full_train_dataset, imbalanced_indices)\n",
    "print(f\"Created imbalanced training set with {len(imbalanced_train_dataset)} images.\")\n",
    "\n",
    "# Address data imbalances\n",
    "# Calculate the weights for `imbalanced_train_dataset`\n",
    "\n",
    "# Get the labels of all the images in the new dataset\n",
    "imbalanced_labels = np.array(full_train_dataset.targets)[imbalanced_train_dataset.indices]\n",
    "\n",
    "# Calculate the number of each category\n",
    "class_counts = np.bincount(imbalanced_labels, minlength=10)\n",
    "print(f\"Imbalanced class counts: {class_counts}\")\n",
    "\n",
    "class_weights = 1.0 / (class_counts + 1e-6)\n",
    "\n",
    "# WeightedRandomSampler\n",
    "print(\"\\n--- Applying Solution 1: WeightedRandomSampler ---\")\n",
    "# Assign a weight to each image in the dataset for its category\n",
    "sample_weights = class_weights[imbalanced_labels]\n",
    "\n",
    "# Create a sampler\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "sampler = WeightedRandomSampler(\n",
    "    weights=sample_weights,\n",
    "    num_samples=len(sample_weights), \n",
    "    replacement=True\n",
    ")\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(\n",
    "    dataset=imbalanced_train_dataset, \n",
    "    batch_size=BATCH_SIZE,\n",
    "    sampler=sampler, \n",
    "    shuffle=False,   \n",
    "    num_workers=4  \n",
    ")\n",
    "\n",
    "print(\"train_loader created using WeightedRandomSampler.\")\n",
    "\n",
    "# ---- WeightedLoss  ----\n",
    "# class_weights_tensor = torch.tensor(class_weights, dtype=torch.float)\n",
    "# print(f\"\\n--- Applying Solution 2: WeightedLoss (DISABLED) ---\")\n",
    "# train_loader = DataLoader(\n",
    "#     dataset=imbalanced_train_dataset,\n",
    "#     batch_size=BATCH_SIZE,\n",
    "#     shuffle=True, # <--- 使用 shuffle, 不使用 sampler\n",
    "#     num_workers=4  \n",
    "# )\n",
    "# print(\"train_loader created using shuffle=True (for WeightedLoss).\")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    dataset=val_subset, \n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False, \n",
    "    num_workers=4\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False, \n",
    "    num_workers=4\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f0c212-b251-4a44-96a8-093ce9ae38d4",
   "metadata": {},
   "source": [
    "### 3. Model define and training\n",
    "During the model training phase, we used transfer learning, loading a pre-trained ResNet-34 model. We trained only the last layer of this pre-trained model (ResNet-34) to quickly complete the cat and dog classification task. The code loops 10 times, evaluating the performance with a validation set after each loop, and finally saving the best-performing model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4e6ea60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using device: cuda\n",
      "\n",
      "  Starting Training (Feature Extraction) on Imbalanced Data  \n",
      "Epoch 1/10 [123s] LR: 1.0e-03\n",
      "  Train Loss: 1.2549 Acc: 0.5870\n",
      "  Val   Loss: 0.7027 Acc: 0.7712\n",
      "  New best model saved (Acc: 0.7712)\n",
      "Epoch 2/10 [121s] LR: 9.8e-04\n",
      "  Train Loss: 0.9263 Acc: 0.6883\n",
      "  Val   Loss: 0.5951 Acc: 0.7990\n",
      "  New best model saved (Acc: 0.7990)\n",
      "Epoch 3/10 [123s] LR: 9.0e-04\n",
      "  Train Loss: 0.8724 Acc: 0.7010\n",
      "  Val   Loss: 0.5903 Acc: 0.7992\n",
      "  New best model saved (Acc: 0.7992)\n",
      "Epoch 4/10 [124s] LR: 7.9e-04\n",
      "  Train Loss: 0.8384 Acc: 0.7086\n",
      "  Val   Loss: 0.5868 Acc: 0.7938\n",
      "Epoch 5/10 [122s] LR: 6.5e-04\n",
      "  Train Loss: 0.8283 Acc: 0.7166\n",
      "  Val   Loss: 0.5493 Acc: 0.8046\n",
      "  New best model saved (Acc: 0.8046)\n",
      "Epoch 6/10 [123s] LR: 5.0e-04\n",
      "  Train Loss: 0.8194 Acc: 0.7150\n",
      "  Val   Loss: 0.5618 Acc: 0.8010\n",
      "Epoch 7/10 [121s] LR: 3.5e-04\n",
      "  Train Loss: 0.7995 Acc: 0.7244\n",
      "  Val   Loss: 0.5506 Acc: 0.8100\n",
      "  New best model saved (Acc: 0.8100)\n",
      "Epoch 8/10 [121s] LR: 2.1e-04\n",
      "  Train Loss: 0.7858 Acc: 0.7306\n",
      "  Val   Loss: 0.5469 Acc: 0.8090\n",
      "Epoch 9/10 [120s] LR: 9.5e-05\n",
      "  Train Loss: 0.8017 Acc: 0.7235\n",
      "  Val   Loss: 0.5422 Acc: 0.8104\n",
      "  New best model saved (Acc: 0.8104)\n",
      "Epoch 10/10 [121s] LR: 2.4e-05\n",
      "  Train Loss: 0.7808 Acc: 0.7304\n",
      "  Val   Loss: 0.5464 Acc: 0.8112\n",
      "  New best model saved (Acc: 0.8112)\n",
      "\n",
      "Training complete. Best validation accuracy: 0.8112\n",
      "Best model weights loaded.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"\\nUsing device: {device}\")\n",
    "model = models.resnet34(weights=models.ResNet34_Weights.IMAGENET1K_V1)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "num_ftrs = model.fc.in_features \n",
    "model.fc = nn.Linear(num_ftrs, 10)\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = optim.Adam(model.fc.parameters(), lr=0.001)\n",
    "NUM_EPOCHS = 10 \n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS)\n",
    "\n",
    "best_val_acc = 0.0 \n",
    "best_model_wts = copy.deepcopy(model.state_dict()) \n",
    "\n",
    "print(\"\\n  Starting Training (Feature Extraction) on Imbalanced Data  \")\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    #   Training Phase  \n",
    "    model.train() \n",
    "    val_subset.dataset.transform = train_transform\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device) \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs) \n",
    "        loss = criterion(outputs, labels) \n",
    "        _, preds = torch.max(outputs, 1) \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "        \n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    epoch_acc = running_corrects.double() / len(train_loader.dataset)\n",
    "    \n",
    "    \n",
    "    model.eval()\n",
    "    val_subset.dataset.transform = test_transform\n",
    "    \n",
    "    val_loss = 0.0\n",
    "    val_corrects = 0\n",
    "    with torch.no_grad(): \n",
    "        for inputs, labels in val_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "            val_corrects += torch.sum(preds == labels.data)\n",
    "            \n",
    "    epoch_val_loss = val_loss / len(val_subset)\n",
    "    epoch_val_acc = val_corrects.double() / len(val_subset)\n",
    "    \n",
    "    val_subset.dataset.transform = train_transform\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f'Epoch {epoch+1}/{NUM_EPOCHS} [{elapsed_time:.0f}s] LR: {scheduler.get_last_lr()[0]:.1e}')\n",
    "    print(f'  Train Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "    print(f'  Val   Loss: {epoch_val_loss:.4f} Acc: {epoch_val_acc:.4f}')\n",
    "\n",
    "    if epoch_val_acc > best_val_acc:\n",
    "        best_val_acc = epoch_val_acc\n",
    "        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        torch.save(model.state_dict(), 'best_model_cifar10_unbalanced.pth')\n",
    "        print(f'  New best model saved (Acc: {epoch_val_acc:.4f})')\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "print(f\"\\nTraining complete. Best validation accuracy: {best_val_acc:.4f}\")\n",
    "model.load_state_dict(best_model_wts)\n",
    "print(\"Best model weights loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1668dd-710c-4889-adee-60e4b4362e13",
   "metadata": {},
   "source": [
    "### 4. Whole model Fine-Tuning\n",
    "\n",
    "To improve out model, we now need to \"unfreeze\" the entire model and let all 34 layers participate in training, but we will use a very small learning rate to avoid destroying the pre-trained knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27917083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model weights loaded, preparing for fine-tuning...\n",
      "Unfreezing all model layers...\n",
      "  Starting Fine-Tuning (LR=1e-05, Epochs=5)  \n",
      "Fine-Tune Epoch 1/5 [176s] LR: 1.0e-05\n",
      "  Train Loss: 0.4758 Acc: 0.8344\n",
      "  Val   Loss: 0.2480 Acc: 0.9168\n",
      "  New best fine-tuned model saved (Acc: 0.9168)\n",
      "Fine-Tune Epoch 2/5 [184s] LR: 9.0e-06\n",
      "  Train Loss: 0.2865 Acc: 0.9012\n",
      "  Val   Loss: 0.1979 Acc: 0.9324\n",
      "  New best fine-tuned model saved (Acc: 0.9324)\n",
      "Fine-Tune Epoch 3/5 [192s] LR: 6.5e-06\n",
      "  Train Loss: 0.2176 Acc: 0.9271\n",
      "  Val   Loss: 0.1852 Acc: 0.9362\n",
      "  New best fine-tuned model saved (Acc: 0.9362)\n",
      "Fine-Tune Epoch 4/5 [193s] LR: 3.5e-06\n",
      "  Train Loss: 0.1901 Acc: 0.9361\n",
      "  Val   Loss: 0.1820 Acc: 0.9376\n",
      "  New best fine-tuned model saved (Acc: 0.9376)\n",
      "Fine-Tune Epoch 5/5 [192s] LR: 9.5e-07\n",
      "  Train Loss: 0.1781 Acc: 0.9392\n",
      "  Val   Loss: 0.1800 Acc: 0.9386\n",
      "  New best fine-tuned model saved (Acc: 0.9386)\n",
      "\n",
      "Fine-tuning complete. Final best validation accuracy: 0.9386\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('best_model_cifar10_unbalanced.pth'))\n",
    "print(\"Best model weights loaded, preparing for fine-tuning...\")\n",
    "\n",
    "NUM_EPOCHS_FT = 5\n",
    "LEARNING_RATE_FT = 1e-5 \n",
    "\n",
    "print(\"Unfreezing all model layers...\")\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "optimizer_ft = optim.AdamW(model.parameters(), lr=LEARNING_RATE_FT)\n",
    "scheduler_ft = optim.lr_scheduler.CosineAnnealingLR(optimizer_ft, T_max=NUM_EPOCHS_FT)\n",
    "\n",
    "print(f\"  Starting Fine-Tuning (LR={LEARNING_RATE_FT}, Epochs={NUM_EPOCHS_FT})  \")\n",
    "\n",
    "for epoch in range(NUM_EPOCHS_FT):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    #   Training Phase  \n",
    "    model.train()\n",
    "    val_subset.dataset.transform = train_transform\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    \n",
    "    for inputs, labels in train_loader: # train_loader 仍然是带 sampler 的\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device) \n",
    "        optimizer_ft.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels) # criterion 仍然是带 (或不带) weights 的\n",
    "        _, preds = torch.max(outputs, 1) \n",
    "        loss.backward() \n",
    "        optimizer_ft.step() \n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "        \n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    epoch_acc = running_corrects.double() / len(train_loader.dataset)\n",
    "    \n",
    "    #   Validation Phase (保持不变)\n",
    "    model.eval()\n",
    "    val_subset.dataset.transform = test_transform\n",
    "    \n",
    "    val_loss = 0.0\n",
    "    val_corrects = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "            val_corrects += torch.sum(preds == labels.data)\n",
    "            \n",
    "    epoch_val_loss = val_loss / len(val_subset)\n",
    "    epoch_val_acc = val_corrects.double() / len(val_subset)\n",
    "    \n",
    "    val_subset.dataset.transform = train_transform\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f'Fine-Tune Epoch {epoch+1}/{NUM_EPOCHS_FT} [{elapsed_time:.0f}s] LR: {scheduler_ft.get_last_lr()[0]:.1e}')\n",
    "    print(f'  Train Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "    print(f'  Val   Loss: {epoch_val_loss:.4f} Acc: {epoch_val_acc:.4f}')\n",
    "\n",
    "    if epoch_val_acc > best_val_acc: \n",
    "        best_val_acc = epoch_val_acc\n",
    "        torch.save(model.state_dict(), 'fine_tuned_best_model_cifar10_unbalanced.pth')\n",
    "        print(f'  New best fine-tuned model saved (Acc: {epoch_val_acc:.4f})')\n",
    "\n",
    "    scheduler_ft.step()\n",
    "\n",
    "print(f\"\\nFine-tuning complete. Final best validation accuracy: {best_val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d5dec0-7c7f-4fe2-883f-005fca039fb9",
   "metadata": {},
   "source": [
    "\n",
    "### 5. Predict test dataset\n",
    "Using our previously trained and saved best model, run predictions on the test set (test_loader) and format the results as labels of 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f5c2f91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Final Test Set Evaluation (on Holdout Set) ---\n",
      "Evaluation complete.\n",
      "\n",
      "Final UNBIASED Accuracy on Cifar-10 Test Set: 0.9334\n",
      "Total Correct: 9334 / 10000\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Starting Final Test Set Evaluation (on Holdout Set) ---\")\n",
    "\n",
    "model.load_state_dict(torch.load('fine_tuned_best_model_cifar10_unbalanced.pth', map_location=device))\n",
    "model.eval() \n",
    "\n",
    "test_corrects = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader: # <--- 使用真正的 test_loader\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        test_corrects += torch.sum(preds == labels)\n",
    "\n",
    "final_accuracy = test_corrects.double() / total\n",
    "print(\"Evaluation complete.\")\n",
    "print(f\"\\nFinal UNBIASED Accuracy on Cifar-10 Test Set: {final_accuracy:.4f}\")\n",
    "print(f\"Total Correct: {test_corrects} / {total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5f4981-bf50-4d53-8fed-e8eecc8b0cab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatglm3-demo",
   "language": "python",
   "name": "chatglm3-demo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
